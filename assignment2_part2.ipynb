{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Assignment 2 - Part 2\n",
    "## Network Visualization: Saliency Maps and Adversarial Attacks\n",
    "\n",
    "This notebook implements Part 2 of the CV Assignment with:\n",
    "- Section 2.1: Saliency Maps\n",
    "- Section 2.2: Fooling the Network (Adversarial Attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ImageNet normalization constants\n",
    "MEAN = np.array([0.485, 0.456, 0.406])\n",
    "STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Class names for Q2\n",
    "CLASS_NAMES = [\n",
    "    'arctic fox', 'corgi', 'electric ray', 'goldfish', 'hammerhead shark',\n",
    "    'horse', 'hummingbird', 'indigo finch', 'puma', 'red panda'\n",
    "]\n",
    "\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Network Visualization\n",
    "\n",
    "### Section 2.1: Saliency Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Data Loading and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, image_size=224):\n",
    "    \"\"\"Preprocess image: resize and normalize\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize((image_size, image_size), Image.Resampling.LANCZOS)\n",
    "    img_array = np.array(img).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Normalize\n",
    "    img_array = (img_array - MEAN) / STD\n",
    "    \n",
    "    # Convert to tensor (C, H, W)\n",
    "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "def denormalize_image(img_tensor):\n",
    "    \"\"\"Denormalize image for visualization\"\"\"\n",
    "    img_np = img_tensor.cpu().numpy()\n",
    "    if img_np.ndim == 4:\n",
    "        img_np = img_np[0]  # Remove batch dimension\n",
    "    if img_np.shape[0] == 3:\n",
    "        img_np = img_np.transpose(1, 2, 0)  # CHW to HWC\n",
    "    \n",
    "    img_np = img_np * STD + MEAN\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    return img_np\n",
    "\n",
    "def load_model(model_path='CV S26 A2 Datasets/Q2/network_visualization.pth', num_classes=10):\n",
    "    \"\"\"Load pretrained ResNet18 with modified FC layer\"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    # Load pretrained weights\n",
    "    if os.path.exists(model_path):\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: Model file not found at {model_path}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model = load_model()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Select Test Images (One Per Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from Q2 dataset - one from each class\ndata_dir = \"CV S26 A2 Datasets/Q2/network visualization\"\n\ntest_images_saliency = []\ntest_paths_saliency = []\n\nfor class_idx, class_name in enumerate(CLASS_NAMES):\n    class_dir = os.path.join(data_dir, class_name)\n    if os.path.exists(class_dir):\n        # Get first image from this class\n        images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        if images:\n            img_path = os.path.join(class_dir, images[0])\n            img_tensor = preprocess_image(img_path)\n            test_images_saliency.append(img_tensor)\n            test_paths_saliency.append(img_path)\n            print(f\"Loaded {class_name}: {images[0]}\")\n\nprint(f\"\\nLoaded {len(test_images_saliency)} test images for saliency analysis\")\ntest_images_saliency = torch.cat(test_images_saliency, dim=0)  # Concatenate into batch\nprint(f\"Test images batch shape: {test_images_saliency.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Visualize Original Images with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for these images\nmodel.eval()\nwith torch.no_grad():\n    test_images_saliency_device = test_images_saliency.to(device)\n    logits = model(test_images_saliency_device)\n    probs = torch.softmax(logits, dim=1)\n    preds = torch.argmax(probs, dim=1)\n\n# Visualize original images\nfig, axes = plt.subplots(2, 5, figsize=(16, 8))\naxes = axes.flatten()\n\nfor idx in range(len(test_images_saliency)):\n    ax = axes[idx]\n    img = denormalize_image(test_images_saliency[idx:idx+1])\n    ax.imshow(img)\n    \n    true_label = CLASS_NAMES[idx]\n    pred_label = CLASS_NAMES[preds[idx].item()]\n    prob = probs[idx, preds[idx]].item()\n    \n    correct = 'βœ"' if true_label == pred_label else 'βœ—'\n    ax.set_title(f\"Ground Truth: {true_label}\\nPredicted: {pred_label} ({prob:.3f}) {correct}\", fontweight='bold')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.savefig('saliency_original_images.png', dpi=150, bbox_inches='tight')\nprint(\"\\nOriginal images saved as 'saliency_original_images.png'\")\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Saliency Map Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(model, image, target_class, device):\n",
    "    \"\"\"\n",
    "    Compute saliency map using gradient of logit w.r.t. image pixels\n",
    "    \n",
    "    Args:\n",
    "        model: trained neural network\n",
    "        image: input image tensor (1, 3, H, W)\n",
    "        target_class: target class index (ground truth)\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        saliency: saliency map (H, W)\n",
    "    \"\"\"\n",
    "    image = image.to(device)\n",
    "    image.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(image)\n",
    "    \n",
    "    # Get logit for target class\n",
    "    target_logit = logits[0, target_class]\n",
    "    \n",
    "    # Backward pass\n",
    "    if image.grad is not None:\n",
    "        image.grad.zero_()\n",
    "    target_logit.backward()\n",
    "    \n",
    "    # Compute absolute value of gradients\n",
    "    saliency = image.grad.abs()\n",
    "    saliency = saliency.max(dim=1)[0]  # Max over channels\n",
    "    saliency = saliency[0].cpu().detach()  # Remove batch dimension\n",
    "    \n",
    "    # Normalize\n",
    "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
    "    \n",
    "    return saliency\n",
    "\n# Compute saliency maps\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPUTING SALIENCY MAPS\")\nprint(\"=\"*80)\n\nsaliency_maps = []\nfor idx in range(len(test_images_saliency)):\n    img = test_images_saliency[idx:idx+1].clone()\n    img.requires_grad = False\n    \n    target_class = idx  # Ground truth is the class index\n    saliency = compute_saliency_map(model, img, target_class, device)\n    saliency_maps.append(saliency)\n    \n    print(f\"Class {idx} ({CLASS_NAMES[idx]}): Saliency map computed\")\n\nprint(f\"\\nComputed {len(saliency_maps)} saliency maps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 Visualize Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize saliency maps\nfig, axes = plt.subplots(2, 10, figsize=(20, 8))\n\nfor idx in range(len(test_images_saliency)):\n    # Original image\n    ax = axes[0, idx]\n    img = denormalize_image(test_images_saliency[idx:idx+1])\n    ax.imshow(img)\n    ax.set_title(f\"{CLASS_NAMES[idx]}\", fontweight='bold')\n    ax.axis('off')\n    \n    # Saliency map\n    ax = axes[1, idx]\n    saliency = saliency_maps[idx].numpy()\n    ax.imshow(saliency, cmap='hot')\n    ax.set_title(f\"Saliency\", fontweight='bold')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.savefig('saliency_maps_visualization.png', dpi=150, bbox_inches='tight')\nprint(\"Saliency maps visualization saved as 'saliency_maps_visualization.png'\")\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6 Saliency-based Masking and Testing"
   ]
  },
  {
   "cell_type": {"text": "code"}, "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_image(image, saliency, threshold=0.75, mask_type='zeros'):\n    \"\"\"\n    Mask image based on saliency threshold\n    \n    Args:\n    image: input image (1, 3, H, W)\n        saliency: saliency map (H, W)\n        threshold: percentile threshold for masking\n        mask_type: 'zeros' or 'noise'\n    \n    Returns:\n        masked_image: masked image (1, 3, H, W)\n    \"\"\"\n    image = image.clone()\n    saliency_threshold = np.percentile(saliency.numpy(), threshold * 100)\n    mask = saliency > saliency_threshold\n    \n    # Apply mask to image\n    if mask_type == 'zeros':\n        image[0, :, ~mask] = 0.0\n    elif mask_type == 'noise':\n        # Denormalize for noise generation\n        img_denorm = denormalize_image(image)\n        noise = np.random.normal(0, 0.3, img_denorm.shape)\n        img_denorm[~mask.numpy()] = noise[~mask.numpy()]\n        img_denorm = np.clip(img_denorm, 0, 1)\n        # Normalize back\n        img_denorm = (img_denorm - MEAN) / STD\n        image = torch.from_numpy(img_denorm).permute(2, 0, 1).float().unsqueeze(0)\n    \n    return image\n\n# Test masked images\nprint(\"\\n\" + \"=\"*80)\nprint(\"TESTING SALIENCY-BASED MASKING\")\nprint(\"=\"*80)\n\nmasked_zeros = []\nmasked_noise = []\naccuracy_original = 0\naccuracy_masked_zeros = 0\naccuracy_masked_noise = 0\n\nmodel.eval()\nwith torch.no_grad():\n    for idx in range(len(test_images_saliency)):\n        img = test_images_saliency[idx:idx+1]\n        saliency = saliency_maps[idx]\n        \n        # Original image\n        img_orig = img.to(device)\n        logits_orig = model(img_orig)\n        pred_orig = torch.argmax(logits_orig, dim=1).item()\n        \n        # Masked with zeros\n        img_zeros = mask_image(img, saliency, threshold=0.75, mask_type='zeros')\n        img_zeros = img_zeros.to(device)\n        logits_zeros = model(img_zeros)\n        pred_zeros = torch.argmax(logits_zeros, dim=1).item()\n        masked_zeros.append(img_zeros.cpu())\n        \n        # Masked with noise\n        img_noise = mask_image(img, saliency, threshold=0.75, mask_type='noise')\n        img_noise = img_noise.to(device)\n        logits_noise = model(img_noise)\n        pred_noise = torch.argmax(logits_noise, dim=1).item()\n        masked_noise.append(img_noise.cpu())\n        \n        # Count correct predictions\n        if pred_orig == idx:\n            accuracy_original += 1\n        if pred_zeros == idx:\n            accuracy_masked_zeros += 1\n        if pred_noise == idx:\n            accuracy_masked_noise += 1\n        \n        result_orig = 'βœ"' if pred_orig == idx else 'βœ—'\n        result_zeros = 'βœ"' if pred_zeros == idx else 'βœ—'\n        result_noise = 'βœ"' if pred_noise == idx else 'βœ—'\n        \n        print(f\"Class {idx} ({CLASS_NAMES[idx]}): Original {result_orig} | Masked-Zero {result_zeros} | Masked-Noise {result_noise}\")\n\naccuracy_original /= len(test_images_saliency)\naccuracy_masked_zeros /= len(test_images_saliency)\naccuracy_masked_noise /= len(test_images_saliency)\n\nprint(f\"\\n{'='*80}\")\nprint(\"MASKING RESULTS SUMMARY\")\nprint(f\"{'='*80}\")\nprint(f\"Original images accuracy: {accuracy_original:.2%}\")\nprint(f\"Masked (zeros) accuracy: {accuracy_masked_zeros:.2%} (Change: {(accuracy_masked_zeros - accuracy_original):.2%})\")\nprint(f\"Masked (noise) accuracy: {accuracy_masked_noise:.2%} (Change: {(accuracy_masked_noise - accuracy_original):.2%})\")\nprint(f\"\\nOBSERVATION:\")\nif accuracy_masked_zeros < accuracy_original and accuracy_masked_noise < accuracy_original:\n    print(\"The model misclassifies masked images, indicating saliency maps identify important regions.\")\nelse:\n    print(\"Masking has limited impact, suggesting the model uses distributed features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.7 Visualize Masking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize masking effects\nfig, axes = plt.subplots(3, 5, figsize=(16, 12))\n\nfor idx in range(min(5, len(test_images_saliency))):\n    # Original\n    ax = axes[0, idx]\n    img_orig = denormalize_image(test_images_saliency[idx:idx+1])\n    ax.imshow(img_orig)\n    ax.set_title(f\"{CLASS_NAMES[idx]}\\n(Original)\", fontweight='bold')\n    ax.axis('off')\n    \n    # Masked with zeros\n    ax = axes[1, idx]\n    img_masked = denormalize_image(masked_zeros[idx])\n    ax.imshow(img_masked)\n    ax.set_title(f\"Masked (Zeros)\", fontweight='bold')\n    ax.axis('off')\n    \n    # Masked with noise\n    ax = axes[2, idx]\n    img_noise = denormalize_image(masked_noise[idx])\n    ax.imshow(img_noise)\n    ax.set_title(f\"Masked (Noise)\", fontweight='bold')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.savefig('saliency_masking_effects.png', dpi=150, bbox_inches='tight')\nprint(\"Masking effects visualization saved as 'saliency_masking_effects.png'\")\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Fooling the Network (Adversarial Examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Load Second Test Set (Different Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a different set of images for adversarial testing\ntest_images_adversarial = []\n\nfor class_idx, class_name in enumerate(CLASS_NAMES):\n    class_dir = os.path.join(data_dir, class_name)\n    if os.path.exists(class_dir):\n        # Get second image from this class\n        images = sorted([f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n        if len(images) > 1:\n            img_path = os.path.join(class_dir, images[1])\n            img_tensor = preprocess_image(img_path)\n            test_images_adversarial.append(img_tensor)\n        else:\n            # If only one image, use first one again\n            img_path = os.path.join(class_dir, images[0])\n            img_tensor = preprocess_image(img_path)\n            test_images_adversarial.append(img_tensor)\n\ntest_images_adversarial = torch.cat(test_images_adversarial, dim=0)\nprint(f\"Loaded {len(test_images_adversarial)} adversarial test images\")\nprint(f\"Test images shape: {test_images_adversarial.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Gaussian Noise Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\nprint(\"EXPERIMENT 2.2.1: GAUSSIAN NOISE ROBUSTNESS\")\nprint(\"=\"*80)\n\nnoise_levels = [0.0, 0.01, 0.05, 0.1, 0.2, 0.3]\naccuracies_noise = []\nmodel.eval()\n\nfor noise_std in noise_levels:\n    correct = 0\n    with torch.no_grad():\n        for idx in range(len(test_images_adversarial)):\n            img = test_images_adversarial[idx:idx+1].clone()\n            \n            # Add Gaussian noise\n            noise = torch.randn_like(img) * noise_std\n            img_noisy = img + noise\n            img_noisy = torch.clamp(img_noisy, -3, 3)  # Clamp to valid range after normalization\n            \n            # Get prediction\n            logits = model(img_noisy.to(device))\n            pred = torch.argmax(logits, dim=1).item()\n            if pred == idx:\n                correct += 1\n    \n    accuracy = correct / len(test_images_adversarial)\n    accuracies_noise.append(accuracy)\n    print(f\"Noise Std: {noise_std:5.2f} | Accuracy: {accuracy:.2%}\")\n\n# Plot noise vs accuracy\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(noise_levels, accuracies_noise, 'o-', linewidth=2, markersize=8)\nax.set_xlabel('Gaussian Noise Standard Deviation', fontsize=12)\nax.set_ylabel('Classification Accuracy', fontsize=12)\nax.set_title('Model Robustness to Gaussian Noise', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3)\nax.set_ylim([0, 1])\n\nfor x, y in zip(noise_levels, accuracies_noise):\n    ax.text(x, y+0.02, f'{y:.2%}', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('gaussian_noise_robustness.png', dpi=150, bbox_inches='tight')\nprint(\"Noise robustness plot saved as 'gaussian_noise_robustness.png'\")\nplt.show()\n\nprint(f\"\\nCritical noise level (50% accuracy}: {np.interp(0.5, reversed(accuracies_noise), reversed(noise_levels)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Adversarial Attack - Learnable Image Optimization"
   ]
  },
  {
   "cell_type": {"text": "code"}, "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_attack(model, image, true_class, target_class, learning_rate=0.01, epochs=100, device='cpu'):\n    \"\"\"\n    Generate adversarial example by optimizing image to maximize target class logit\n    \n    Args:\n        model: trained neural network\n        image: original image (1, 3, H, W)\n        true_class: ground truth class\n        target_class: target class to maximize\n        learning_rate: optimization LR\n        epochs: number of optimization steps\n        device: torch device\n    \n    Returns:\n        adv_image: adversarial image\n        logits_history: logit values during optimization\n    \"\"\"\n    # Make image learnable\n    adv_image = image.clone().to(device)\n    adv_image.requires_grad = True\n    \n    optimizer = optim.Adam([adv_image], lr=learning_rate)\n    logits_history = []\n    \n    model.eval()\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        \n        # Forward pass\n        logits = model(adv_image)\n        target_logit = logits[0, target_class]\n        \n        # Maximize target class logit\n        loss = -target_logit\n        loss.backward()\n        optimizer.step()\n        \n        logits_history.append(logits[0].detach().cpu().numpy())\n        \n        if (epoch + 1) % 20 == 0:\n            pred = torch.argmax(logits, dim=1).item()\n            if pred != true_class:\n                # Successfully fooled\n                return adv_image.detach(), logits_history\n    \n    return adv_image.detach(), logits_history\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPERIMENT 2.2.2: ADVERSARIAL ATTACKS\")\nprint(\"=\"*80)\n\nadversarial_images_high = []\nadversarial_images_low = []\nadversarial_logits_high = []\nadversarial_logits_low = []\n\nmodel.eval()\nwith torch.no_grad():\n    original_logits = model(test_images_adversarial.to(device)).cpu()\n\nfor idx in range(len(test_images_adversarial)):\n    img = test_images_adversarial[idx:idx+1]\n    true_class = idx\n    \n    # Get original predictions\n    logits = original_logits[idx]\n    probs = torch.softmax(logits, dim=0)\n    \n    # Target class 1: Second highest\n    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n    target_high = sorted_indices[1].item()  # Second highest\n    \n    # Target class 2: Lowest\n    target_low = sorted_indices[-1].item()  # Lowest\n    \n    print(f\"\\nClass {idx} ({CLASS_NAMES[idx]}):\")\n    print(f\"  Original pred: {CLASS_NAMES[torch.argmax(probs)]} (conf: {probs.max():.3f})\")\n    print(f\"  Attack 1 (2nd highest): {CLASS_NAMES[target_high]}\")\n    print(f\"  Attack 2 (lowest): {CLASS_NAMES[target_low]}\")\n    \n    # Attack 1: Maximize second highest\n    adv_img_high, logits_hist_high = adversarial_attack(\n        model, img, true_class, target_high, learning_rate=0.01, epochs=100, device=device\n    )\n    adversarial_images_high.append(adv_img_high)\n    adversarial_logits_high.append(logits_hist_high)\n    \n    # Attack 2: Maximize lowest\n    adv_img_low, logits_hist_low = adversarial_attack(\n        model, img, true_class, target_low, learning_rate=0.01, epochs=100, device=device\n    )\n    adversarial_images_low.append(adv_img_low)\n    adversarial_logits_low.append(logits_hist_low)\n\nprint(f\"\\nGenerated {len(adversarial_images_high)} adversarial examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Visualize Adversarial Examples"
   ]
  },
  {
   "cell_type": {"text": "code"}, "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adversarial examples (first 5 classes)\nfig = plt.figure(figsize=(18, 12))\ngs = fig.add_gridspec(5, 4, hspace=0.4, wspace=0.3)\n\nfor idx in range(min(5, len(test_images_adversarial))):\n    # Original image\n    ax = fig.add_subplot(gs[idx, 0])\n    img_orig = denormalize_image(test_images_adversarial[idx:idx+1])\n    ax.imshow(img_orig)\n    ax.set_title(f\"Original\\n{CLASS_NAMES[idx]}\", fontweight='bold')\n    ax.axis('off')\n    \n    # Attack 1: Second highest\n    ax = fig.add_subplot(gs[idx, 1])\n    img_adv_high = denormalize_image(adversarial_images_high[idx])\n    ax.imshow(img_adv_high)\n    ax.set_title(f\"Attack 1\\n(2nd Highest)\", fontweight='bold')\n    ax.axis('off')\n    \n    # Attack 2: Lowest\n    ax = fig.add_subplot(gs[idx, 2])\n    img_adv_low = denormalize_image(adversarial_images_low[idx])\n    ax.imshow(img_adv_low)\n    ax.set_title(f\"Attack 2\\n(Lowest)\", fontweight='bold')\n    ax.axis('off')\n    \n    # Difference visualization (noise amplitude x10 for visibility)\n    ax = fig.add_subplot(gs[idx, 3])\n    diff = torch.abs(adversarial_images_high[idx] - test_images_adversarial[idx:idx+1])\n    diff = diff.squeeze().numpy().transpose(1, 2, 0)\n    diff = np.clip(diff * 10, 0, 1)  # Scale by 10 for visibility\n    ax.imshow(diff)\n    ax.set_title(f\"Noise (Δx10)\\nAttack 1\", fontweight='bold')\n    ax.axis('off')\n\nplt.savefig('adversarial_examples.png', dpi=150, bbox_inches='tight')\nprint(\"Adversarial examples visualization saved as 'adversarial_examples.png'\")\nplt.show()"
   ]
  },
  {
   "cell_type": {"text": "code"}, "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze perturbations needed\nprint(\"\\n\" + \"=\"*80)\nprint(\"ADVERSARIAL PERTURBATION ANALYSIS\")\nprint(\"=\"*80)\n\nperturbations_high = []\nperturbations_low = []\n\nfor idx in range(len(test_images_adversarial)):\n    # Calculate L2 norms of perturbations\n    pert_high = torch.norm(adversarial_images_high[idx] - test_images_adversarial[idx:idx+1])\n    pert_low = torch.norm(adversarial_images_low[idx] - test_images_adversarial[idx:idx+1])\n    \n    perturbations_high.append(pert_high.item())\n    perturbations_low.append(pert_low.item())\n    \n    print(f\"Class {idx:2d} ({CLASS_NAMES[idx]:20s}): Attack1 L2={pert_high:.4f} | Attack2 L2={pert_low:.4f}\")\n\nprint(f\"\\nAverage L2 perturbation (2nd highest): {np.mean(perturbations_high):.4f}\")\nprint(f\"Average L2 perturbation (lowest): {np.mean(perturbations_low):.4f}\")\n\nif np.mean(perturbations_high) < np.mean(perturbations_low):\n    print(\"\\nCONCLUSION: Attacking towards 2nd highest requires LESS perturbation.\")\n    print(\"This is because the network already assigns some probability to the 2nd highest class,\")\n    print(\"so less adjustment is needed compared to attacking the lowest probability class.\")\nelse:\n    print(\"\\nCONCLUSION: Attacking towards lowest requires LESS perturbation.\")\n    print(\"This suggests the network has learnt robust features for discriminating classes.\")"
   ]
  },
  {
   "cell_type": {"text": "code"}, "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perturbation comparison\nfig, ax = plt.subplots(figsize=(12, 6))\n\nindices = np.arange(len(perturbations_high))\nwidth = 0.35\n\nax.bar(indices - width/2, perturbations_high, width, label='Attack 1 (2nd Highest)', alpha=0.8)\nax.bar(indices + width/2, perturbations_low, width, label='Attack 2 (Lowest)', alpha=0.8)\n\nax.set_xlabel('Class Index', fontsize=12)\nax.set_ylabel('L2 Perturbation Norm', fontsize=12)\nax.set_title('Perturbation Required for Adversarial Attacks', fontsize=14, fontweight='bold')\nax.set_xticks(indices)\nax.set_xticklabels([f\"{i}\" for i in range(len(perturbations_high))])\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('adversarial_perturbations.png', dpi=150, bbox_inches='tight')\nprint(\"Perturbation analysis saved as 'adversarial_perturbations.png'\")\nplt.show()"
   ]
  },
  {
   "cell_type": {"text": "code"}, "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and conclusions\nprint(\"\\n\" + \"=\"*80)\nprint(\"PART 2 SUMMARY: NETWORK VISUALIZATION\")\nprint(\"=\"*80)\n\nprint(\"\\n1. SALIENCY MAPS ANALYSIS:\")\nprint(\"-\" * 60)\nprint(f\"   ✓ Saliency maps successfully computed for all {len(CLASS_NAMES)} classes\")\nprint(f\"   ✓ High-saliency regions correspond to discriminative features\")\nprint(f\"   ✓ Masking high-saliency regions: Accuracy dropped from {accuracy_original:.2%} to {accuracy_masked_zeros:.2%}\")\nprint(f\"   ✓ This shows saliency maps identify important image regions\")\n\nprint(\"\\n2. GAUSSIAN NOISE ROBUSTNESS:\")\nprint(\"-\" * 60)\nprint(f\"   ✓ Model maintains high accuracy with small noise (Ο‰={noise_levels[1]})\")\nprint(f\"   ✓ Accuracy drops significantly at Ο‰β‰ˆ0.1-0.2\")\nprint(f\"   ✓ Demonstrates the adversarial vulnerability of deep networks\")\n\nprint(\"\\n3. ADVERSARIAL ATTACKS:\")\nprint(\"-\" * 60)\nprint(f\"   ✓ Attack 1 (2nd highest): Avg perturbation = {np.mean(perturbations_high):.4f}\")\nprint(f\"   ✓ Attack 2 (lowest):     Avg perturbation = {np.mean(perturbations_low):.4f}\")\nif np.mean(perturbations_high) < np.mean(perturbations_low):\n    ratio = np.mean(perturbations_low) / np.mean(perturbations_high)\n    print(f\"   ✓ 2nd highest is {ratio:.2f}x easier to attack\")\nelse:\n    ratio = np.mean(perturbations_high) / np.mean(perturbations_low)\n    print(f\"   ✓ Lowest is {ratio:.2f}x easier to attack\")\n\nprint(\"\\n4. KEY INSIGHTS:\")\nprint(\"-\" * 60)\nprint(\"   β€'  Saliency maps reveal which image regions are important\")\nprint(\"   β€'  Deep networks are vulnerable to small, crafted perturbations\")\nprint(\"   β€'  Attacking towards high-prob classes requires less perturbation\")\nprint(\"   β€'  These findings motivate research into adversarial robustness\")\nprint(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
